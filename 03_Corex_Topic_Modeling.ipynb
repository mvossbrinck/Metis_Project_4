{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize   \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lemmatizer Class to use in CountVectorizer\n",
    "##### I got the code and understanding of how to use it from here https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Clean Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Episode_Split</th>\n",
       "      <th>Episode_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>Hello. Hi. My name is Leslie Knope, and I work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>Well, one of the funner things that we do here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>The Parks Department has so many programs. Jer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>I don't believe it. Oh, my God. It's real. Hey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>In a town as old as Pawnee, there's a lot of h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Character Episode Episode_Split  \\\n",
       "0  Leslie Knope   s1e01         s1e01   \n",
       "1  Leslie Knope   s1e02         s1e02   \n",
       "2  Leslie Knope   s1e03         s1e03   \n",
       "3  Leslie Knope   s1e04         s1e04   \n",
       "4  Leslie Knope   s1e05         s1e05   \n",
       "\n",
       "                                        Episode_Text  \n",
       "0  Hello. Hi. My name is Leslie Knope, and I work...  \n",
       "1  Well, one of the funner things that we do here...  \n",
       "2  The Parks Department has so many programs. Jer...  \n",
       "3  I don't believe it. Oh, my God. It's real. Hey...  \n",
       "4  In a town as old as Pawnee, there's a lot of h...  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open and check data source\n",
    "with open('Leslie_all.pickle','rb') as read_file:\n",
    "    leslie_episodes = pickle.load(read_file)\n",
    "\n",
    "leslie_episodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Episode_Split</th>\n",
       "      <th>Episode_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>Hello   name  Leslie Knope and  work for the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>Well one  the funner things that   here  Pawne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>The Parks Department has  many programs Jerry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>don believe    God  real Hey Hey Hello Boys  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leslie Knope</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>town  old  Pawnee there  lot  history  every...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Character Episode Episode_Split  \\\n",
       "0  Leslie Knope   s1e01         s1e01   \n",
       "1  Leslie Knope   s1e02         s1e02   \n",
       "2  Leslie Knope   s1e03         s1e03   \n",
       "3  Leslie Knope   s1e04         s1e04   \n",
       "4  Leslie Knope   s1e05         s1e05   \n",
       "\n",
       "                                        Episode_Text  \n",
       "0  Hello   name  Leslie Knope and  work for the P...  \n",
       "1  Well one  the funner things that   here  Pawne...  \n",
       "2  The Parks Department has  many programs Jerry ...  \n",
       "3   don believe    God  real Hey Hey Hello Boys  ...  \n",
       "4    town  old  Pawnee there  lot  history  every...  "
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove digits, any words less than 3 characters, and punctuation\n",
    "leslie_episodes['Episode_Text'] = leslie_episodes['Episode_Text'].str.replace('\\d+', '') # for digits\n",
    "leslie_episodes['Episode_Text'] = leslie_episodes['Episode_Text'].str.replace(r'(\\b\\w{1,2}\\b)', '') # for words\n",
    "leslie_episodes['Episode_Text'] = leslie_episodes['Episode_Text'].str.replace('[^\\w\\s]', '') # for punctuation \n",
    "leslie_episodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Episode_Split</th>\n",
       "      <th>Episode_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>Tonight is our next monthly community outreach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>Uh, sure, Paul. What can I do for you? Yeah, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>No comment. Hey, Haverford, maybe one day you'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>Go to jail? What's going on? Put it in an emai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>The only reason anybody's going to this thing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Character Episode Episode_Split  \\\n",
       "0  Ron Swanson   s1e01         s1e01   \n",
       "1  Ron Swanson   s1e02         s1e02   \n",
       "2  Ron Swanson   s1e03         s1e03   \n",
       "3  Ron Swanson   s1e04         s1e04   \n",
       "4  Ron Swanson   s1e05         s1e05   \n",
       "\n",
       "                                        Episode_Text  \n",
       "0  Tonight is our next monthly community outreach...  \n",
       "1  Uh, sure, Paul. What can I do for you? Yeah, a...  \n",
       "2  No comment. Hey, Haverford, maybe one day you'...  \n",
       "3  Go to jail? What's going on? Put it in an emai...  \n",
       "4  The only reason anybody's going to this thing ...  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open and check data source\n",
    "with open('Ron_all.pickle','rb') as read_file:\n",
    "    ron_episodes = pickle.load(read_file)\n",
    "\n",
    "ron_episodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Episode_Split</th>\n",
       "      <th>Episode_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>s1e01</td>\n",
       "      <td>Tonight  our next monthly community outreach p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>s1e02</td>\n",
       "      <td>sure Paul What can   for you Yeah absolutely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>s1e03</td>\n",
       "      <td>comment Hey Haverford maybe one day you figur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>s1e04</td>\n",
       "      <td>jail What going  Put    email Let not blow t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Swanson</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>s1e05</td>\n",
       "      <td>The only reason anybody going  this thing  bec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Character Episode Episode_Split  \\\n",
       "0  Ron Swanson   s1e01         s1e01   \n",
       "1  Ron Swanson   s1e02         s1e02   \n",
       "2  Ron Swanson   s1e03         s1e03   \n",
       "3  Ron Swanson   s1e04         s1e04   \n",
       "4  Ron Swanson   s1e05         s1e05   \n",
       "\n",
       "                                        Episode_Text  \n",
       "0  Tonight  our next monthly community outreach p...  \n",
       "1   sure Paul What can   for you Yeah absolutely ...  \n",
       "2   comment Hey Haverford maybe one day you figur...  \n",
       "3    jail What going  Put    email Let not blow t...  \n",
       "4  The only reason anybody going  this thing  bec...  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove digits, any words less than 3 characters, and punctuation\n",
    "ron_episodes['Episode_Text'] = ron_episodes['Episode_Text'].str.replace('\\d+', '') # for digits\n",
    "ron_episodes['Episode_Text'] = ron_episodes['Episode_Text'].str.replace(r'(\\b\\w{1,2}\\b)', '') # for words\n",
    "ron_episodes['Episode_Text'] = ron_episodes['Episode_Text'].str.replace('[^\\w\\s]', '') # for punctuation \n",
    "ron_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Corex Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leslie Final Corex Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually created stop words\n",
    "commonwords = ['use', 'own', 'barely', 'bottom']\n",
    "\n",
    "names = ['leslie', 'knope', 'ron', 'swanson', 'ben', 'wyatt', 'april', 'ludgate', 'tom', 'haverford', \n",
    "         'ann', 'perkins', 'andy', 'dwyer', 'jerry', 'gergich', 'donna', 'meagle', 'tommy', 'mark', 'brendanawicz'] \n",
    "\n",
    "stop_words = list(set(commonwords + names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=20000, \n",
    "                             stop_words=stop_words, \n",
    "                             tokenizer = LemmaTokenizer(),\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(leslie_episodes.Episode_Text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n",
      "0: park,department,director,recreation,live,choice,summer,further,subcommittee,neighborhood\n",
      "1: government,federal,wheel,turning,intern,ready,brainstorming,themselves,moved,truck\n"
     ]
    }
   ],
   "source": [
    "# Run topic model\n",
    "topic_model = ct.Corex(n_hidden=2, words=words,\n",
    "                       max_iter=200, verbose = False, seed = 1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=leslie_episodes.Episode_Text, \n",
    "                anchors=[['park'],\n",
    "                         ['government']]\n",
    "                         , anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ron Final Corex Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually created stop words and nltk stopwords\n",
    "commonwords = ['become', 'new', 'even']\n",
    "\n",
    "names = ['leslie', 'knope', 'ron', 'swanson', 'ben', 'wyatt', 'april', 'ludgate', 'tom', 'haverford', \n",
    "         'ann', 'perkins', 'andy', 'dwyer', 'jerry', 'gergich', 'donna', 'meagle', 'tommy', 'mark', 'brendanawicz'] \n",
    "\n",
    "nltk_stopwords =  stopwords.words('english')\n",
    "\n",
    "stop_words = list(set(names + commonwords + nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madelinevossbrinck/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# Transform data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=20000, \n",
    "                             stop_words=stop_words, \n",
    "                             tokenizer = LemmaTokenizer(),\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(ron_episodes.Episode_Text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: park,drive,want,job,killed,nothing,girl,better,person,would\n",
      "1: government,office,taxpayer,meet,large,crap,stand,dollar,hired,wasting\n"
     ]
    }
   ],
   "source": [
    "# Run topic model\n",
    "topic_model = ct.Corex(n_hidden=2, words=words,\n",
    "                       max_iter=200, verbose = False, seed = 1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=ron_episodes.Episode_Text, \n",
    "                anchors=[['park'],\n",
    "                         ['government']]\n",
    "                         , anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional CorEx Topic Models\n",
    "##### This is not a comprehensive list of all that were tried, just some of the alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Models for Leslie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually created stop words and nltk stopwords\n",
    "commonwords = ['wheel', 'turning','use', 'barely', 'bottom', 'except']\n",
    "\n",
    "names = ['leslie', 'knope', 'ron', 'swanson', 'ben', 'wyatt', 'april', 'ludgate', 'tom', 'haverford', \n",
    "         'ann', 'perkins', 'andy', 'dwyer', 'jerry', 'gergich', 'donna', 'meagle', 'tommy', 'mark', 'brendanawicz'] \n",
    "\n",
    "nltk_stopwords =  stopwords.words('english')\n",
    "\n",
    "stop_words = list(set(nltk_stopwords + names + commonwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=20000, \n",
    "                             stop_words=stop_words, tokenizer = LemmaTokenizer(),\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(leslie_episodes.Episode_Text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: park,department,director,live,pit,year,candidate,system,subcommittee,closed\n",
      "1: government,summer,clear,intern,recreation,prepared,community,someday,deputy,cake\n"
     ]
    }
   ],
   "source": [
    "# Run topic model\n",
    "topic_model = ct.Corex(n_hidden=2, words=words,\n",
    "                       max_iter=200, verbose = False, seed = 1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=leslie_episodes.Episode_Text, \n",
    "                anchors=[['park'],\n",
    "                         ['government']]\n",
    "                         , anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually created stop words and nltk stopwords\n",
    "commonwords = ['use', 'extra']\n",
    "\n",
    "names = ['leslie', 'knope', 'ron', 'swanson', 'ben', 'wyatt', 'april', 'ludgate', 'tom', 'haverford', \n",
    "         'ann', 'perkins', 'andy', 'dwyer', 'jerry', 'gergich', 'donna', 'meagle', 'tommy', 'mark', 'brendanawicz'] \n",
    "\n",
    "\n",
    "nltk_stopwords =  stopwords.words('english')\n",
    "\n",
    "stop_words = list(set(names + nltk_stopwords + commonwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=20000, \n",
    "                             stop_words=stop_words, tokenizer = LemmaTokenizer(),\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(leslie_episodes.Episode_Text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: park,director,department,recreation,choice,water,councilwoman,year,neighborhood,sell\n",
      "1: government,wheel,wave,intern,cake,bottom,barely,open,zero,notice\n"
     ]
    }
   ],
   "source": [
    "# Run topic model\n",
    "topic_model = ct.Corex(n_hidden=2, words=words,\n",
    "                       max_iter=200, verbose = False, seed = 1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=leslie_episodes.Episode_Text, \n",
    "                anchors=[['park'],\n",
    "                         ['government']]\n",
    "                         , anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Models for Ron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually created stop words and nltk stopwords\n",
    "commonwords = ['cream', 'cheese', 'want', 'new', 'large', 'made', 'small', 'throat', 'old', 'feel' ,'must']\n",
    "\n",
    "names = ['leslie', 'knope', 'ron', 'swanson', 'ben', 'wyatt', 'april', 'ludgate', 'tom', 'haverford', \n",
    "         'ann', 'perkins', 'andy', 'dwyer', 'jerry', 'gergich', 'donna', 'meagle', 'tommy', 'mark', 'brendanawicz'] \n",
    "\n",
    "nltk_stopwords =  stopwords.words('english')\n",
    "\n",
    "stop_words = list(set(names + nltk_stopwords + commonwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=20000, \n",
    "                             stop_words=stop_words, tokenizer = LemmaTokenizer(),\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(ron_episodes.Episode_Text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: park,job,complete,need,department,run,six,drive,person,help\n",
      "1: government,taxpayer,stand,offering,budget,buddy,wasting,tax,show,win\n"
     ]
    }
   ],
   "source": [
    "# Run topic model\n",
    "topic_model = ct.Corex(n_hidden=2, words=words,\n",
    "                       max_iter=200, verbose = False, seed = 1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=ron_episodes.Episode_Text, \n",
    "                anchors=[['park'],\n",
    "                         ['government']]\n",
    "                         , anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually created stop words and nltk stopwords\n",
    "commonwords = ['new', 'expect', 'tomorrow', 'even', 'must', 'told']\n",
    "\n",
    "names = ['leslie', 'knope', 'ron', 'swanson', 'ben', 'wyatt', 'april', 'ludgate', 'tom', 'haverford', \n",
    "         'ann', 'perkins', 'andy', 'dwyer', 'jerry', 'gergich', 'donna', 'meagle', 'tommy', 'mark', 'brendanawicz'] \n",
    "\n",
    "nltk_stopwords =  stopwords.words('english')\n",
    "\n",
    "stop_words = list(set(commonwords + names + nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=20000, \n",
    "                             stop_words=stop_words, tokenizer = LemmaTokenizer(),\n",
    "                             binary=True)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(ron_episodes.Episode_Text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: park,want,speak,station,federal,fit,definitely,throw,complicated,invite\n",
      "1: government,crap,office,taxpayer,made,right,city,think,buddy,tax\n"
     ]
    }
   ],
   "source": [
    "# Run topic model\n",
    "topic_model = ct.Corex(n_hidden=2, words=words,\n",
    "                       max_iter=200, verbose = False, seed = 1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=ron_episodes.Episode_Text, \n",
    "                anchors=[['park'],\n",
    "                         ['government']]\n",
    "                         , anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
